name: scrape-journalism-data

on:  
  workflow_dispatch:
  schedule:
    - cron:  '30 17 * * *'

jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
    # Step 1: Prepare the environment
    - name: Check out this repo
      uses: actions/checkout@v2
      with:
        fetch-depth: 0
   
    # Step 2: Install requirements, so Python script can run
    - name: Install requirements
      run: python -m pip install pandas requests selenium beautifulsoup4 webdriver-manager tqdm
    # Step 3    
    - name: Run script to create main csv
      run: python wapo-job-search-tool.py     
    # Step 4    
    - name: Run script to create main csv
      run: python npr-job-search-tool.py  
    # Step 5
    - name: Commit and push if it changed
      run: |-
        git config user.name "Automated"
        git config user.email "actions@users.noreply.github.com"
        git add -A
        timestamp=$(date -u)
        git commit -m "Latest data: ${timestamp}" || exit 0
        git push
